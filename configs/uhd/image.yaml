# ==== Preprocessing ==== #
preproc_name: init
debug: False

data_root: /mnt/tsukuyomi/uhd-eeg/
# ica_data_root: /home/sensho/brain2face/driving_ica/

start_subj: 0
end_subj: 22

seq_len: 3 # segment length in seconds

# ---- Face ---- #
fps: 30

face_extractor:
  mode: mesh
  output_size: 256
  eyes_dist_mult: 4
  movement_alert_thresh: 50 # px

# ---- Brain ---- #
num_channels: 128
brain_orig_sfreq: 250 # Hz
brain_resample_sfreq: 120 # Hz
brain_filter_low: 1.0 # Hz
brain_filter_high: 60 # Hz
clamp_lim: 20 # values after robust scaling
baseline_len: 0.5 # sec
# shift_brain: False
# shift_len: 150 # ms

eeg_load_bufsize: 1000 # samples


# ==== Training ==== #
project_name: brain2face_uhd
train_name: test # Will be updated when it's sweep

dataset: UHD # UHD / YLabECoG / StyleGAN

montage_path: brain2face/utils/montages/electrodes_uhd.xml

F: 512 # CLIP embedding dimension

face:
  type: static # static (image) or dynamic (video)
  reduction: extract # extract or mean. Only for face.type=static
  encoded: True # Whether to use pretrained CLIP model
  clip_model: ViT-B/32 # Used if pretrained is False

vit:
  image_size: 256
  patch_size: 32
  depth: 6
  heads: 16
  mlp_dim: 2048
  dropout: 0.1
  emb_dropout: 0.1

# Brain encoder
D1: 270
D2: 320
K: 32
d_drop: 0.3 # Channel distance for spatial dropout
# NOTE: setting these 2 is the key that BrainEncoder downsamples EEG from 120Hz to 30Hz
final_ksize_stride: 1
head_activation: True

split: deep # shallow / deep / subject_random / subject_each
# subject_random: picks 20% sessions for test, without considering subjects' identity
# subject_each: each subject has one or two test sessions (currently not working)

train_ratio: 0.8
chance: False
batch_size: 64
lr: 0.005
lr_scheduler: multistep # cosine or multistep
lr_multistep_mlstns: [0.4, 0.6, 0.8, 0.9]
lr_step_gamma: 0.5
epochs: 500
reduction: mean
init_temperature: 5.0

seed: 1234

cuda_id: 0

sweep_count: 20
sweep_config:
  method: bayes
  metric:
    name: test_top10_acc
    goal: maximize
  parameters:
    split:
      values: [deep, shallow]
    d_drop:
      values: [0.1, 0.3, 0.5]
    face.encoded:
      values: [True, False]
    head_activation:
      values: [True, False]
    final_ksize_stride:
      values: [1, 2]
    face.reduction:
      values: [extract, mean]

eval:
  split: shallow
  d_drop: 0.1
  face.encoded: True
  head_activation: False
  final_ksize_stride: 2
  face.reduction: extract