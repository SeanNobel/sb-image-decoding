# ==== Preprocessing ==== #
preproc_name: init
debug: False

data_root: /mnt/tsukuyomi/uhd-eeg/
# ica_data_root: /home/sensho/brain2face/driving_ica/
start_subj: 0
end_subj: 22

seq_len: 3 # segment length in seconds

# ---- Face ---- #
fps: 30

face_extractor:
  mode: mesh
  output_size: 256
  eyes_dist_mult: 4
  movement_alert_thresh: 50 # px

# ---- Brain ---- #
num_channels: 128
brain_orig_sfreq: 250 # Hz
brain_resample_sfreq: 120 # Hz
brain_filter_low: 1.0 # Hz
brain_filter_high: 60 # Hz
clamp_lim: 20 # values after robust scaling
baseline_len: 0.5 # sec
shift_brain: False
shift_len: 150 # ms

eeg_load_bufsize: 1000 # samples


# ==== Training ==== #
project_name: brain2face_uhd
train_name: deep-split

dataset: Brain2FaceUHDDataset

F: 512 # CLIP embedding dimension

face:
  type: image # image or video
  pretrained: True
  clip_model: ViT-B/32 # Used if pretrained is False

vit:
  image_size: 256
  patch_size: 32
  depth: 6
  heads: 16
  mlp_dim: 2048
  dropout: 0.1
  emb_dropout: 0.1

# Brain encoder
D1: 270
D2: 320
K: 32
d_drop: 0.3 # Channel distance for spatial dropout
# NOTE: setting these 2 is the key that BrainEncoder downsamples EEG from 120Hz to 30Hz
final_kernel_size: 2
final_stride: 2

split: deep # shallow / deep / subject_random / subject_each
# subject_random: picks 20% sessions for test, without considering subjects' identity
# subject_each: each subject has one or two test sessions (currently not working)

train_ratio: 0.8
chance: False

batch_size: 64
lr: 0.005
lr_scheduler: multistep # cosine or multistep
lr_multistep_mlstns: [0.4, 0.6, 0.8, 0.9]
lr_step_gamma: 0.5
epochs: 1000
reduction: mean
init_temperature: 5.0

montage_path: brain2face/utils/gTecUtils/montages/electrodes_uhd.xml

seed: 1234

cuda_id: 0

use_wandb: False