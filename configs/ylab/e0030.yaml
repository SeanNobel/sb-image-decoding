# ==== Preprocessing ==== #
preproc_name: upto500Hz_onlyAUr
debug: False

ecog_data_root: /storage/data/ECoG/E0030/Raw/Mat2k/
face_data_path: /work/data/ECoG/E0030/E0030_*.csv
sync_data_path: /work/data/ECoG/rishido/E0030sync_trigger.csv
processing_sheet_path: /work/data/ECoG/E0030/E0030ProcessingSheet.xlsx

start_subj: 0
end_subj: 18

segment_in_preproc: False # for sweeping over segment length
seq_len: 3 # segment length in seconds

# ---- Face ---- #
fps: 30

# ---- Brain---- #
brain_orig_sfreq: 2000
brain_resample_sfreq: 500 # Hz
brain_filter_low: 0.1 # Hz
brain_filter_high: 250 # Hz
clamp_lim: 20 # values after robust scaling
baseline_len: 0.5 # sec
# shift_brain: False
# shift_len: 150 # ms


# ==== Training ==== #
project_name: brain2face_ylab
train_name: test # Will be updated when it's a sweep

dataset: YLabECoG # UHD / YLabECoG / StyleGAN

montage_path: brain2face/utils/montages/E0030/loc.npy
loc_random: False # whether to use montage

F: 709

face:
  type: dynamic # static (image) or dynamic (video)
  reduction: extract # extract or mean. Only for face.type=static
  encoded: True # Face features are already encoded by OpenFace

# Brain Encoder
D1: 270
D2: 320
K: 32
d_drop: 0.3 # Channel distance for spatial dropout
# NOTE: setting these 2 is the key that BrainEncoder downsamples EEG from 120Hz to 30Hz
final_ksize_stride: 2
head_activation: True

split: shallow # shallow / deep / subject_random / subject_each
# subject_random: picks 20% sessions for test, without considering subjects' identity
# subject_each: each subject has one or two test sessions (currently not working)

train_ratio: 0.8
chance: False
batch_size: 64
lr: 0.005
lr_scheduler: multistep # cosine or multistep
lr_multistep_mlstns: [0.4, 0.6, 0.8, 0.9]
lr_step_gamma: 0.5
epochs: 500
reduction: mean
init_temperature: 5.0

seed: 1234

cuda_id: 0

sweep_count: 2
sweep_config:
  method: grid
  metric:
    name: test_top10_acc
    goal: maximize
  parameters:
    loc_random:
      values: [True, False]
    # split:
    #   values: [deep, shallow]
    # d_drop:
    #   values: [0.1, 0.3, 0.5]
    # head_activation:
    #   values: [True, False]
    # final_ksize_stride:
    #   values: [1, 2]
    # face.reduction:
    #   values: [extract, mean]
    # face.type:
    #   values: [static, dynamic]

eval:
  split: shallow
  d_drop: 0.1
  face.encoded: True
  head_activation: False
  final_ksize_stride: 2
  face.reduction: extract