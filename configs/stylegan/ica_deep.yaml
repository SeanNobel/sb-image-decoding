# ==== Preprocessing ==== #
preproc_name: ica_eog_rm
debug: False

data_root: /mnt/neurotech_nas/driving-game/ # /home/sensho/brain2face/assets/example_data/
ica_data_root: /home/sensho/brain2face/data/raw/driving_ica/
start_subj: 0
end_subj: 8

seq_len: 3 # segment length in seconds

# ---- Face ---- #
fps: 30

face_extractor:
  mode: mesh
  output_size: 256
  eyes_dist_mult: 4
  movement_alert_thresh: 50 # px

stylegan:
  model_path: /home/sensho/brain2face/encoder4editing/weights/e4e_ffhq_encode.pt

# ---- Brain ---- #
brain_orig_sfreq: 250 # Hz
brain_resample_sfreq: 120 # Hz
brain_filter_low: 1.0 # Hz
brain_filter_high: 60 # Hz
clamp_lim: 20 # values after robust scaling
baseline_ratio: 0.2 # ratio compared to segment length
# shift_brain: False
# shift_len: 150 # ms


# ==== Training ==== #
project_name: brain2face_stylegan
train_name: ica_deep

dataset: Brain2FaceStyleGANDataset
encode_face: False # Already encoded in StyleGAN latent

split: deep # shallow / deep / subject_random / subject_each
# subject_random: picks 20% sessions for test, without considering subjects' identity
# subject_each: each subject has one or two test sessions (currently not working)

train_ratio: 0.8
chance: False

batch_size: 64
lr: 0.005
lr_scheduler: cosine # cosine or multistep
lr_multistep_mlstns: [0.4, 0.6, 0.8, 0.9]
lr_step_gamma: 0.5
epochs: 500
reduction: mean
init_temperature: 5.0

montage_path: brain2face/utils/gTecUtils/montages/montage_EEGonly_32ch.xml

cuda_id: 0
seed: 1234
use_wandb: True


# ==== Architecture ==== #
# Common
F: 512 # 1024

# Brain
D1: 270
D2: 320
K: 32
d_drop: 0.3 # Channel distance for spatial dropout
# NOTE: setting these 2 is the key that BrainEncoder downsamples EEG from 120Hz to 30Hz
final_kernel_size: 2
final_stride: 2

