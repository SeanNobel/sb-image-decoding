# ==== Preprocessing ==== #
preproc_name: 1_paper

thingsmeg_dir: /mnt/tsukuyomi/things-meg/
meg_dir: ${thingsmeg_dir}derivatives/preprocessed/

things_dir: /mnt/tsukuyomi/things/osfstorage/THINGS/
images_dir: ${things_dir}Images/
metadata_dir: ${things_dir}Metadata/
preprocessed_data_dir: data/preprocessed/thingsmeg/

skip_meg: False
skip_images: True

# brain_filter_low: 0.1 # Hz
# brain_filter_high: 250 # Hz
brain_resample_sfreq: 120 # Hz
clamp_lim: 5.0 # Standard deviation

# ==== Architecture ==== #
F: 768 # CLIP embedding dimension

vision:
  pretrained: True
  model: ViT # For face.pretrained=False
  pretrained_model: ViT-L/14 # For face.pretrained=True
  encoded: False # False is basically only for StyleGAN
  
vision_encoder:
  image_size: ${image_size}
  patch_size: 32
  dim: ${F}
  depth: 6
  heads: 16
  mlp_dim: 2048
  dropout: 0.1
  emb_dropout: 0.1

brain_encoder: eegnet

num_channels: 271
n_mel: ${F} # 512
t_mel: 300
k1: 60 # 960
k2: 6 # 96
F1: 32
F2: 64 
D: 2
p1: 2 # 40
p2: 5 # 80
dr1: 0.60
dr2: 0.30
num_channels_per_patch: 16
use_dilation: true
stride1: 2
stride2: 2
num_conv_time_layers: 3
num_conv_emb_layers: 3
k_div: 2

# spatial_attention: True

temporal_aggregation: original # original or affine or pool. This is somewhat overlapping with reduce_time, but this is for model and reduce_time is for data.
final_ksize: 1
final_stride: 1
biases:
  linear_reduc_time: True

vq_brain: False
vq:
  type: raw # raw or (temporally) aggregated 
  num_embeds: 256
  num_concepts: 8 # Only for aggregated vq. Should correspond to the minimal number of concepts to explain an image with text.
  alpha: 1.0 # How much to weight regularization terms against CLIP loss.
  beta: 0.25 # How much to weight encoder regularization term against codebook loss.

reduce_time: True

# ==== Training ==== #
project_name: nd_things_clip
train_name: eegnet_large-test
# debug: False

# seq_onset: 0.0
seq_len: 1.41 # Original number of samples at 200Hz is 281, and 169 at 120Hz.

dataset: ThingsMEG
# image_size: 256

# montage_path: brain2face/utils/montages/things_meg.npy
# layout: ch_locations_2d # DynamicChanLoc2d

large_test_set: True

batch_size: 128
lr: 3.0e-4 # 0.005
lr_scheduler:  # cosine or multistep
lr_multistep_mlstns: [0.4, 0.6, 0.8, 0.9]
lr_step_gamma: 0.5
epochs: 500
reduction: mean
clip_temp_init: 5.0
clip_temp_learn: True
clip_temp_min: 0.1
clip_temp_max: 10.0

acc_topk: [1, 5]

accum_grad: False

patience: 20

chance: False

test_with_whole: True

plot_latents: False

num_workers: 8
seed: 1234
cuda_id: 1

sweep_count: 4
sweep_config:
  method: grid
  metric:
    name: test_top5_acc
    goal: maximize
  parameters:
    # clip_temp_init:
    #   value: 4.3
    # clip_temp_learn:
    #   value: False
    t_mel:
      values: [100, 200]
    dr1:
      values: [0.6, 0.2]
    # chance:
    #   value: True
    # temporal_aggregation:
    #   values: [original]
    # downsample:
    #   values: [False]
    # split:
    #   values: [mixed_shallow, mixed_deep]
    # ksizes.conv_block:
    #   values: [3, 5, 7]
    # d_drop:
    #   values: [0.4]
    # p_drop:
    #   values: [0.1, 0.2, 0.3]
    # vq_brain:
    #   values: [False, True]
    # vq.type:
    #   values: [aggregated]
    # vq.num_embeds:
    #   values: [64, 256, 16]
    # vq.alpha:
    #   values: [0.001, 0.01, 0.1]
    # vq.beta:
    #   values: [0.25, 1.0]

# ==== Evaluation ==== #
eval:
  seq_len: 0.5

as_h5: True
for_webdataset: False