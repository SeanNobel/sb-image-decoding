# ==== Preprocessing ==== #
preproc_name: 1_paper

thingsmeg_dir: /mnt/tsukuyomi/things-meg/
meg_dir: ${thingsmeg_dir}derivatives/preprocessed/

things_dir: /mnt/tsukuyomi/things/osfstorage/THINGS/
images_dir: ${things_dir}Images/
metadata_dir: ${things_dir}Metadata/
preprocessed_data_dir: data/preprocessed/thingsmeg/

skip_meg: False
skip_images: True

# brain_filter_low: 0.1 # Hz
# brain_filter_high: 250 # Hz
brain_resample_sfreq: 120 # Hz
clamp_lim: 5.0 # Standard deviation

# ==== Architecture ==== #
F: 768 # CLIP embedding dimension

vision:
  pretrained: True
  model: ViT # For face.pretrained=False
  pretrained_model: ViT-L/14 # For face.pretrained=True
  encoded: False # False is basically only for StyleGAN
  
vision_encoder:
  image_size: ${image_size}
  patch_size: 32
  dim: ${F}
  depth: 6
  heads: 16
  mlp_dim: 2048
  dropout: 0.1
  emb_dropout: 0.1

brain_encoder: brain_encoder

num_channels: 271
D1: 270
D2: 320
K: 32
spatial_attention: True
conv_block: dilated_conv # dilated_conv or inception
num_conv_blocks: 2
d_drop: 0.3 # Dropout rate for spatial-attention
p_drop: 0.3 # Dropout rate for conv blocks
downsample: False # Can be a list of bools with length num_conv_blocks
final_ksize: 1
final_stride: 1
temporal_aggregation: affine # original or affine or pool. This is somewhat overlapping with reduce_time, but this is for model and reduce_time is for data.
head_activation: True
ksizes:
  conv_block: 3
biases:
  conv_block: True
  conv_subj_sa: True
  linear_reduc_time: True

vq_type: affine # null / original / affine (= vqtorch basic) / group / residual
vq_aggregated: False
vq_num_embeds: 512
vq_num_concepts: 8 # Only for aggregated vq. Should correspond to the minimal number of concepts to explain an image with text.
vq_use_ema: True
vq_alpha: 1.0 # How much to weight regularization terms against CLIP loss.
vq_beta: 0.0 # How much to weight encoder regularization term against codebook loss.
# When using EMA for codebook, beta needs to be zero. https://arxiv.org/abs/2305.08842
vq_gamma: 0.99 # Decay for EMA of codebook.
vq_epsilon: 1e-5 # Epsilon for EMA of codebook.

# vqtorch
vq_kmeans_init: False # Whether to use kmeans++ init
vq_norm: null # Noralization for input vectors
vq_cb_norm: null # Normalization for codebook vectors
vq_affine_lr: 10.0 # lr scale for affine parameters
vq_sync_nu: 0.2 # Codebook synchronization contribution
vq_replace_freq: 0 # Frequency to replace dead codes
# group
vq_groups: 1 # Number of groups for group VQ
vq_share: True # If True, codebook is shared for each sub-vector.

drop_mode: dropout

reduce_time: True

# ==== Loss ==== #
loss: circleclip

reduction: mean

push_negative: False

use_high_categories: False

clip_temp_init: 5.0
clip_temp_learn: True
clip_temp_min: 
clip_temp_max: 

clip_margin_init: 0.1
clip_margin_learn: False
clip_margin_min:
clip_margin_max:

cosface_alpha: 1.0

nnclip_k: 10
nnclip_symmetric: True
nnclip_support_size: 20000
nnclip_alpha: 0.5

# ==== Training ==== #
project_name: nd_things_clip
train_name: large_test
# debug: False

# seq_onset: 0.0
seq_len: 1.41 # Original number of samples at 200Hz is 281, and 169 at 120Hz.

dataset: ThingsMEG
# image_size: 256

montage_path: brain2face/utils/montages/things_meg.npy
layout: ch_locations_2d # DynamicChanLoc2d

large_test_set: True

batch_size: 128
lr: 3.0e-4 # 0.005
lr_scheduler: null # cosine or multistep
lr_multistep_mlstns: [0.4, 0.6, 0.8, 0.9]
lr_step_gamma: 0.5
epochs: 1000

acc_topk: [1, 5]

accum_grad: False

patience: 80

chance: False

test_with_whole: True

plot_latents: True

num_workers: 4
seed: 1234
cuda_id: 0

sweep_count: 24
sweep_config:
  name: vqtorch_init
  method: grid
  metric:
    name: test_top5_acc
    goal: maximize
  parameters:
    vq_affine_lr:
      values: [10.0, 1.0, 0.1]
    vq_sync_nu:
      values: [0.2, 0.5]
    vq_replace_freq:
      values: [0, 20]

    # use_high_categories:
    #   values: [False, True]

    # push_negative:
    #   values: [False, True]

    # clip_margin_init:
    #   values: [1.0, 10.0, 100.0]
    # cosface_alpha:
    #   values: [1.0]

    # drop_mode:
    #   values: [dropblock, dropout1d, dropout]

    # loss:
    #   values: [nnclip]
    # nnclip_k:
    #   values: [4, 12]
    # nnclip_symmetric:
    #   values: [True]
    # nnclip_support_size:
    #   values: [20000, 200]
    # nnclip_alpha:
    #   values: [0.2, 1.0]

    # spatial_attention:
    #   values: [True]
    # temporal_aggregation:
    #   values: [original, affine]
    # num_conv_blocks:
    #   values: [5, 2]
    # d_drop:
    #   values: [0.1, 0.3]
    #   # distribution: uniform
    #   # min: 0.1
    #   # max: 0.7
    # p_drop:
    #   values: [0.1, 0.3, 0.6]
    #   # distribution: uniform
    #   # min: 0.1
    #   # max: 0.7

    # seq_onset:
    #   values: [0.0, 0.5, 1.0, 1.5]
    # seq_len:
    #   values: [0.5, 0.3, 0.1]

    # layout:
    #   values: [DynamicChanLoc2d, ch_locations_2d]
    # ksizes.conv_block:
    #   values: [3, 5, 7]

    # vq_type:
    #   values: [raw, aggregated]
    vq_num_embeds:
      values: [512, 2048]
    # vq_num_concepts:
    #   values: [8, 32, 128]
    # vq_use_ema:
    #   values: [True]
    # vq_alpha:
    #   values: [0.001, 0.01, 0.1]
    # vq_beta:
    #   values: [0.05, 0.25, 1.0]
    # vq_gamma:
    #   values: [0.99, 0.95]

    # learn_temperature:
    #   values: [False]
    # biases.conv_block:
    #   values: [True, False]
    # biases.conv_subj_sa:
    #   values: [True, False]
    # biases.linear_reduc_time:
    #   values: [True, False]

# ==== Evaluation ==== #
eval:
  seq_len: 0.5

as_h5: True
for_webdataset: False