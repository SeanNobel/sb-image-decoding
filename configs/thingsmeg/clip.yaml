# ==== Preprocessing ==== #
preproc_name: 3_caption # 2_huggingface

thingsmeg_dir: /mnt/tsukuyomi/things-meg/
meg_dir: ${thingsmeg_dir}derivatives/preprocessed/

things_dir: /mnt/tsukuyomi/things/osfstorage/THINGS/
images_dir: ${things_dir}Images/
metadata_dir: ${things_dir}Metadata/

root: /home/sensho/brain2face/
preprocessed_data_dir: ${root}data/preprocessed/thingsmeg/

skip_meg: True
skip_images: True
skip_texts: False
caption: True

# brain_filter_low: 0.1 # Hz
# brain_filter_high: 250 # Hz
brain_resample_sfreq: 120 # Hz
clamp_lim: 5.0 # Standard deviation

# ==== Architecture ==== #
F: 768 # 768 for OpenAI original / 1024 for Huggingface
orig_F: 768

align_to: vision # vision / text
align_tokens: cls # all / cls / mean / list of those
orig_clip_tokens: 1 # 257
num_clip_tokens: 1

vision:
  pretrained: True
  model: ViT # For face.pretrained=False
  pretrained_model: ViT-L/14
  # openai/clip-vit-large-patch14 (Huggingface) or ViT-L/14 (OpenAI)
  encoded: False # False is basically only for StyleGAN
vision_quantize: False
vision_quantize_lr: 1.0e-4 # For when training the quantizer with a separate optimizer.
gumbel_init_temp: 2.0
gumbel_min_temp: 0.5
gumbel_temp_decay: 0.9995

brain_encoder: brain_encoder

num_channels: 271
D1: 270
D2: 320
D3: 2048
K: 32
spatial_attention: True
ignore_subjects: False
blocks: conformer # dilated_conv / inception / transformer. Can be a list of strings with length num_blocks.
num_blocks: 2
d_drop: 0.1 # Dropout rate for spatial-attention
p_drop: 0.1 # Dropout rate for conv blocks
downsample: False # Can be a list of bools with length num_blocks
final_ksize: 1
final_stride: 1
temporal_aggregation: affine # original or affine or pool. This is somewhat overlapping with reduce_time, but this is for model and reduce_time is for data.
head_activation: True
conv_block_ksize: 3
depthwise_ksize: 31
transformer_heads: 4
pos_enc: sine_abs # learn / sine_abs / sine_rel / rotary

# Where of the model to apply vector quantization.
vq: null # null / middle1 (before blocks) / middle2 (after blocks) / end
vq_type: gumbel # original / affine (= vqtorch basic) / group / residual
vq_aggregated: False
vq_num_embeds: 2048
vq_num_concepts: 16 # Only for aggregated vq. Should correspond to the minimal number of concepts to explain an image with text.
vq_alpha: 1.0 # How much to weight vq loss(es) against CLIP loss.
vq_beta: 0.3 # How much to weight encoder regularization term against codebook loss.
# NOTE that this beta is (1 - beta) in vqtorch paper.
vq_affine_lr: 0.1 # lr scale for affine parameters
# EMA. When using EMA for codebook, beta should be zero? https://arxiv.org/abs/2305.08842
vq_use_ema: False
vq_gamma: 0.99 # Decay for EMA of codebook.
vq_epsilon: 1e-5 # Epsilon for EMA of codebook.
# vqtorch
vq_kmeans_init: False # Whether to use kmeans++ init
vq_norm: null # Noralization for input vectors
vq_cb_norm: null # Normalization for codebook vectors
vq_sync_nu: 0.2 # Codebook synchronization contribution
vq_replace_freq: 20 # Frequency to replace dead codes
vq_alternate: False # Whether to alternate between task loss and vq loss
# NOTE that vq_beta will be overwritten by 0.0 when vq_alternate is True.
vq_groups: 4 # Number of groups for group VQ
vq_share: False # If True, codebook is shared for each sub-vector.
# lucidrains
vq_num_quantizers: 16 # Number of quantizers to use

dann: False
dann_scale: 1.0

drop_mode: dropout

reduce_time: False

# ==== Loss ==== #
loss: clip
reduction: mean
impl_type: 0

lambd: 0.75 # Weighting of CLIP and MSE loss

use_negative: False

use_high_categories: False

clip_temp_init: 5.0 # This value is exponentiated in the loss.
clip_temp_learn: True
clip_temp_min: null
clip_temp_max: null

clip_margin_init: 0.1
clip_margin_learn: False
clip_margin_min: null
clip_margin_max: null

cosface_alpha: 1.0

circle_relax: 0.0

positive_threshold: 0.8 # Threshold of cosine similarity (for Additional Positives CLIP)

nnclip_k: 10
nnclip_symmetric: True
nnclip_support_size: 20000
nnclip_alpha: 0.5

leclip_alpha: 0.0
orclip_alpha: 0.1
klclip_alpha: 0.1
kl_beta: 1.0 # Same meaning as alpha but using beta after variational inference.
nrclip_alpha: 0.1

vae: null # ps / vmf / normal / null
vae_zdim: 10
sample_l: 2

subspace_downs: [2, 3, 4, 4]

# ==== Training ==== #
project_name: nd_things_clip
train_name: small_test

# seq_onset: 0.0
seq_len: 1.41 # Original number of samples at 200Hz is 281, and 169 at 120Hz.

dataset: ThingsMEG
# image_size: 256

montage_path: ${root}nd/utils/montages/things_meg.npy
layout: ch_locations_2d # DynamicChanLoc2d

large_test_set: False # True

batch_size: 128
lr: 3.0e-4 # 0.005
lr_scheduler: null # cosine or multistep
lr_multistep_mlstns: [0.4, 0.6, 0.8, 0.9]
lr_step_gamma: 0.5
epochs: 1000

acc_topk: [1, 5]

accum_grad: False

patience: 15

chance: False

test_with_whole: True

plot_latents: False

num_workers: 8
seed: 1234
cuda_id: 1

sweep_count: 3
sweep_config:
  name: subspace-clip-crate # hypersphere-vae # variational-clip # additional-positives-clip # large-batch-size # orthogonality # crate # more-heads # conv-vs-conformer # large-entropy-clip_vision-quantize # margin-on-positives # gumbel-vq-brain # vision-quantize # _from-all-tokens # adaptive-margin-clip_large # adaptive-clip
  method: grid
  metric:
    name: test_top5_acc
    goal: maximize
  parameters:
    # loss:
    #   value: normregclip
    # nrclip_alpha:
    #   values: [0.01, 0.0]

    loss:
      value: subspaceclip
    subspace_downs:
      values: [[2, 3, 4, 4]]
    blocks:
      value: crate
    transformer_heads:
      values: [4, 8, 16]
    num_blocks:
      value: 4

    # vae:
    #   values: [vmf, ps]
    # vae_zdim:
    #   values: [10, 100, 768]
    # sample_l:
    #   value: 4
    # kl_beta:
    #   value: 2.0

    # loss:
    #   value: variationalclip
    # kl_beta:
    #   values: [1.0, 10.0, 100.0]

    # loss:
    #   value: klclip
    # klclip_alpha:
    #   values: [1.0, 5.0, 0.1]

    # batch_size:
    #   value: 8
    # num_blocks:
    #   values: [2, 4]

    # blocks:
    #   value: crate
    # num_blocks:
    #   value: 8
    # transformer_heads:
    #   values: [8, 16]

    # loss:
    #   value: orclip
    # clip_temp_learn:
    #   value: False
    # clip_temp_init:
    #   value: 2.0
    # orclip_alpha:
    #   values: [5.0, 0.5]

    # blocks:
    #   values: [conformer, dilated_conv]
      
    # loss:
    #   value: leclip
    # leclip_alpha:
    #   values: [-0.5, -0.1, 0.1, 0.5]
    # impl_type:
    #   values: [0]

    # chance:
    #   values: [False, True]

    # vq:
    #   values: [middle1, middle2]
    # vq_type:
    #   value: gumbel
    # vq_groups:
    #   values: [8]
    # gumbel_init_temp:
    #   value: 2.0
    # gumbel_min_temp:
    #   values: [0.2]
    # gumbel_temp_decay:
    #   values: [0.9995]
    # vq_alpha:
    #   value: 0.1

    # vision_quantize:
    #   value: True
    # vq_groups:
    #   value: 16
    # gumbel_min_temp:
    #   values: [0.5]
    # gumbel_temp_decay:
    #   values: [0.999995, 0.9995]
    # # preproc_name:
    # #   value: 2_huggingface
    # # F:
    # #   value: 1024
    # # align_tokens:
    # #   value: all
    # # orig_clip_tokens: 
    # #   value: 257

    # loss:
    #   values: [adaptiveclip]
    # # clip_margin_init:
    # #   values: [0.0, 0.2, 0.4]
    # use_negative:
    #   value: True
    # impl_type:
    #   values: [2, 3]

    # loss:
    #   value: amclip
    # clip_margin_init:
    #   value: 0.1
    # clip_temp_init:
    #   values: [4.5]
    # clip_temp_learn:
    #   value: False
    # # use_negative:
    # #   value: True
    # # num_blocks:
    # #   value: 6
    # # D1:
    # #   value: 540
    # # D2:
    # #   value: 640
    # # K:
    # #   value: 64

    # loss:
    #   value: apclip
    # positive_threshold:
    #   values: [0.8, 0.9, 0.95, 0.98]
    # batch_size:
    #   value: 512

    # loss:
    #   value: arcfaceclip
    # use_negative:
    #   value: True
    # clip_margin_init:
    #   values: [0.05, 0.1, 0.2]
    # clip_temp_learn:
    #   value: False
    # clip_temp_init:
    #   values: [4.0, 3.0]
    # num_blocks:
    #   values: [4, 2]

    # loss:
    #   value: cosfaceclip
    # clip_margin_init:
    #   values: [0.2, 0.0]
    # use_negative:
    #   values: [False, True]

    # loss:
    #   value: circleclip
    # clip_temp_init:
    #   values: [4.5, 5.5, 6.2]
    # clip_temp_learn:
    #   value: False
    # circle_relax:
    #   values: [0.9, 0.4]
    # impl_type:
    #   values: [0, 1]

    # loss:
    #   values: [geomclip]
    # impl_type:
    #   values: [18,19]
    # clip_temp_init:
    #   value: 2.0

    # plot_latents:
    #   values: [True]

    # preproc_name:
    #   values: [3_caption]
    # align_to:
    #   values: [vision]
    # align_tokens:
    #   values: [cls]

    # depthwise_ksize:
    #   values: [31, 17, 9, 5]
    
    # pos_enc:
    #   value: sine_rel

    # vq:
    #   values: [middle1]
    # vq_type:
    #   values: [lu_residual]

    # num_blocks:
    #   values: [4]

    # ignore_subjects:
    #   values: [False, True]

    # dann:
    #   values: [False, True]

    # clip_temp_learn:
    #   values: [False]
    # clip_temp_init:
    #   values: [2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8]

    # circle_relax:
    #   values: [0.2, 0.1, 0.0, -0.1]

    # blocks:
    #   values: [
    #     [dilated_conv, dilated_conv, transformer, transformer],
    #     [dilated_conv, transformer, transformer],
    #     [dilated_conv, dilated_conv, transformer],
    #   ]

    # transformer_heads:
    #   values: [4, 8]

    # lambd:
    #   values: [0.0, 0.25, 0.5, 0.75, 1.0]

    # vq_num_quantizers:
    #   values: [16, 32]
    # vq_groups:
    #   values: [8, 4]
    # vq_share:
    #   values: [False, True]

    # vq_alternate:
    #   values: [False]

    # D2:
    #   values: [640, 320] # 320

    # vq_affine_lr:
    #   values: [0.1]
    # vq_sync_nu:
    #   values: [0.2, 0.5]
    # vq_replace_freq:
    #   values: [20]

    # loss:
    #   values: [clip]

    # clip_margin_init:
    #   values: [0.1]
    # clip_margin_learn:
    #   values: [True]

    # use_high_categories:
    #   values: [False, True]

    # push_negative:
    #   values: [False, True]

    # cosface_alpha:
    #   values: [1.0]

    # drop_mode:
    #   values: [dropblock, dropout1d, dropout]

    # nnclip_k:
    #   values: [4, 12]
    # nnclip_symmetric:
    #   values: [True]
    # nnclip_support_size:
    #   values: [20000, 200]
    # nnclip_alpha:
    #   values: [0.2, 1.0]

    # spatial_attention:
    #   values: [True]
    # temporal_aggregation:
    #   values: [original, affine]

    # d_drop:
    #   values: [0.1, 0.3] # 0.3
    #   # distribution: uniform
    #   # min: 0.1
    #   # max: 0.7
    # p_drop:
    #   values: [0.1, 0.3] # 0.3
    #   # distribution: uniform
    #   # min: 0.1
    #   # max: 0.7

    # seq_onset:
    #   values: [0.0, 0.5, 1.0, 1.5]
    # seq_len:
    #   values: [0.5, 0.3, 0.1]

    # layout:
    #   values: [DynamicChanLoc2d, ch_locations_2d]
    # ksizes.conv_block:
    #   values: [3, 5, 7]

    # vq_type:
    #   values: [raw, aggregated]
    # vq_num_embeds:
    #   values: [2048]
    # vq_num_concepts:
    #   values: [8, 32, 128]
    # vq_use_ema:
    #   values: [True]
    # vq_alpha:
    #   values: [1.0]
    # vq_beta:
    #   values: [0.1, 0.3, 0.5]
    # vq_gamma:
    #   values: [0.99, 0.95]

    # learn_temperature:
    #   values: [False]

# ==== Evaluation ==== #
eval:
  loss: normregclip
  nrclip_alpha: 0.01

  # batch_size: 8
  # num_blocks: 2

  # blocks: crate
  # num_blocks: 8
  # transformer_heads: 8

  # clip_temp_learn: False
  # clip_temp_init: 2
  # loss: orclip
  # orclip_alpha: 0.5

  # blocks: conformer

  # chance: False
  # large_test_set: False

  # preproc_name: 3_caption
  # align_tokens: mean

  # num_blocks: 4
  # vq: middle1 # null
  # vq_type: lu_residual

  # vq_num_quantizers: 16
  # loss: clip
  # pos_enc: sine_abs

save_vision: False
as_h5: True
for_webdataset: False